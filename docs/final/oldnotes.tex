\section{Project Idea}
\subsection{Scope/Topic} 
We're looking into file hosting systems such as Dropbox, which are popular tools for remote file storage and backup, collaborative work, and file sharing.
\subsection{Problem} 
These services/systems work well with centralized storage, but keeping all data at a central server or group of servers:
\begin{enumerate}
\item Increases risk of privacy issues
\item Central point of failure

[FILL IN more from SOUP and elsewhere]
\end{enumerate}
\subsection{Solution}
One solution to this problem would be to distribute data storage across all users by storing replicas on individual user machines.
Such a distributed file hosting system must handle many things: replication strategy, node departures and failures, reads/writes, etc.

To distribute data storage in such a manner requires a distributed and scalable replication strategy.

\subsection{Questions}
(Given that nodes do not know global state----and are dealing with a dynamic distributed system--nodes may come and go, nodes may crash, multiple users may work on the same data)
\begin{enumerate}
\item Timing/Synchronicity:

-How do we handle distributed updates of replicas, or rather how do we synchronize updates?

-How do we synchronize updates

\item Availability vs Network Overhead:

-How do we handle node failures?  How do we update data when a node returns?

-How do we ensure that a node gets updated/current data?

\item Group membership:

-How do we keep track of which nodes have replicas?
\end{enumerate}

\subsection{Significance}
To the best of our knowledge, these strategies have not been comparatively evaluated with respect to distributed file hosting systems.

\subsection{Assumptions}
Our assumptions for this project include:
\begin{enumerate}
\item \emph{Reliable network}: The channels will not fail, or at least channel failures are handled by some error detection scheme (TCP).
\item \emph{Conflict resolution}: Take most recent update
\item \emph{Security}: SHA, PKI [fill in from SOUP]
\item \emph{Event ordering}: vector clock
\end{enumerate}

\section{Related Work}
(synchronicity/replication strategies--conservative or optimistic--in DS, issues with DropBox, open source revision|file hosting)

\subsection{Issues with Centralized File Hosting Systems}
(cite dark and exposing papers)

\subsection{Replica Maintanence}
There has been a considerable amount of work in replica maintenance~\cite{chun2006replica,pu1991replica,ford2010availability}.

Ford: availability in a cloud storage system given different system parameters.
NOT decentralized storage environment.
%TODO find more, also check out methodology

Chun: replica algorithms should 1) focus on durability, a less expensive and more useful goal than availability; 2) durability algorithms must create new copies of data faster than failures destroy them; and 3) more replicas helps tolerate bursts of failures but not increasing likelihood of disk failure.
Chun focuses on durability, but we need high availability due to need to keep data in local repository current.

Pu: Discusses various replica control mechanisms which use epsilon-copy serializability (ESR), a correctness criterion that allows asynchronous maintenance of mutual consistency of replicated data. ESR allows inconsistent data to be read, but requires data to eventually converge to 1SR state.
"Standard correctness criterion for coherency control such as 1-copy serializability (1SR) are hard to attain with asynchronous coherency control."
Pu describes and analyzes these various replica control methods, but does not evaluate them in a specific use case such as [file hosting | revision] systems.

\subsection{Synchronization Strategies}
(cite textbook, and data replication chapter)
\begin{itemize}
\item \textbf{Read Once, Write All (ROWA):} ROWA systems allow read operations to merely fetch the requested data from the most convenient location, while write operations are required to update all replicas. 
Read operations are therefore guaranteed to be correct, but if replica-storing node fails during a write operation, then that write operation will fail. (PE: necessarily true?)
\item \textbf{Read Once, Write All-Available (ROWA-A):} ROWA-A read operations are the same as in ROWA, but write operations only affect available replicas. 
This allows writes to subsets of nodes and better handles changes in node availability, but could compromise the correctness of subsequent read requests.
\item \textbf{Quorum Based (QB):} QB systems allow write operations to subsets of nodes without compromising correctness and consistency. An acting node must collect votes from other nodes until the sum of those votes exceeds predefined read or write quorums.  Read and write quorums are defined such that there is always a non-null intersection between any two quorum sets.
\end{itemize}

\section{Design}
\subsection{Client}
Clients are thin--they have local data storage and the broker program, which handles their read and write requests.
We implement clients as individual processes with local storage and a database to track timestamps and revision numbers.
Clients watch their repository directory for any changes (additions, deletions, modifications).\newline\newline
Upon initialization:
\begin{enumerate}
\item update DB
\item start observer
\item connect to broker
\item push entire directory as request
\end{enumerate}
Upon change event:
\begin{enumerate}
\item append change to queue
\item send request to broker
\end{enumerate}
Upon broker request ack:
\begin{enumerate}
\item dequeue change
\item send change to broker
\end{enumerate}
Upon broker pushback:
\begin{enumerate}
\item update DB
\item receive change (lock?)
\end{enumerate}

\subsection{Broker}
The purpose of the broker in our scheme is to handle read and write requests of the individual clients. 
Group management: the broker keeps record of all clients participating in the system.
A broker keeps its own copy of the revision numbers.
\newline\newline
Upon receiving a connection:
\begin{enumerate}
\item recv request
\item lock
\item compare request to local DB (revision numbers)
\item if valid respond with ack, else:
  \begin{enumerate}
    \item respond with push
    \item RETRIEVE
    \item PUSHBACK
\end{enumerate}
\item unlock
\end{enumerate}
Upon receiving client changes:
\begin{enumerate}
\item update DB
\item PUSH
\end{enumerate}

\subsection{Messages}
CONNECT

REQUEST

ACK

RETRIEVE

PUSHBACK

PUSH

(fill in)
\subsection{Network}
Clients and brokers use TCP stream sockets to communicate with one another.

\subsection{Issues Resolved}
What if a client dies? Just consider it gone by the system.\newline\newline
What if a client dies after it makes a request? Drop the request.\newline\newline
Client re-emerges (death + reentry) with updated files? Use local database to check timestamps, update local database, and push changes.\newline\newline
How to handle group managament? At the broker.\newline\newline
What if the broker dies? All clients save state..upon reentering the broker has fresh local storage

\subsection{NOTES}
%details: difference between add/delete, how handle, watchdog, etc
Watchdog and Asynchchat = observer pattern/asynch commun.

\section{Evaluation}
[THIS ALL NEEDS TO BE UPDATED]
\begin{enumerate}
\item Testing Platform:
\item Parameters:
Centralized broker vs decentralized broker.

Different levels of node churn.

Different replication strategies.
\item Metrics:
\begin{itemize}
\item Available:

-network overhead?

-failed pull attempts?
\item Reliable:

-number of successful up-to-date pulls?
\end{itemize}
\item Scalability:
\end{enumerate}
\section{Timeline}
\begin{enumerate}
\item This weekend: complete literature review (synchronicity/replication strategies in DS, issues with DropBox, open source revision|file hosting)

-Each of us: research 1 of each

-Fill in 1 paragraph summary of any literature, add any ideas to notes

\item Week 8: Design
  \begin{enumerate}
    \item Build client
    \item Implement storage
    \item Create network definition (graph, edges are cost)
    \item Implement central broker (define policies)
    \item Implement Distributed broker
  \end{enumerate}
\item Week 9 (Thanksgiving): Testing/Evaluation
  
  TO CODE:
  \begin{enumerate}
    \item Client: local storage (sqlite)
    \item Client: handle init (update db, start observer, push all)
    \item Broker: local storage (mysql? sqlite?)
    \item Broker: handle request (either accept/respond or pushback current)
    \item Broker: request queue
    \item TIMERS/TIMEOUT
    \item Centralized server?
    \item Testing environment
  \end{enumerate}
\item Week 10: Presentation, Write paper
\item Final Exam Week: Write paper, turn in paper, have multiple celebratory beers
\end{enumerate}

\section{UPDATE}
\subsection{(1) Brief synopsis of our project:}
We're looking into file hosting systems such as Dropbox, which are popular tools for remote file storage and backup, collaborative work, and file sharing.
These systems rely on centralized storage, which have inherent privacy issues and present a central point of failure.
One solution to this problem would be to distribute data storage across all users by storing replicas on individual user machines.
Such a distributed file hosting system must handle many things: replication strategy, node departures and failures, reads/writes, etc.
So, given users with dynamic online times (and possible failures), can we provide a distributed file hosting system that allows online reads/writes and high data availability?

\subsection{(2) What have we done:}
  a] We've done literature review on distributed storage systems, available replication/synchronization strategies, and similar work on evaluating the above
    b] We've handled a number of design decisions involving:
          -communication model: TCP sockets
                -architecture: clients with local storage, and a broker network that handles client read/write operations (and necessary replication) and group membership
                      -race conditions: how to handle competing operations/how to order operations, how to lock and unlock and queue requests
                            -failures: how to handle when a node goes offline/crashes, how to update a node upon reentry/recovery
                                  -???evaluation: metrics=availability,network overhead,correctness and parameters=node churn, ???
                                    c] We've implemented:
                                          -client: watches local directory and communicates with broker (pushes/pulls updates)
                                                -broker: communicates with clients, handles updates
                                                (3) Where are we now:
                                                  a] We're finishing up client-broker communication:
                                                        -synchronization policies, local timestamps, locks
                                                          b] We're designing/building the test environment
                                                          (4) What we would like to discuss:
                                                            a] What to measure?
                                                                  -policies?
                                                                        -vs centralized server?
                                                                          b] How to measure?
                                                                                -availability?
                                                                                      -correctness?
                                                                                            -overhead?
